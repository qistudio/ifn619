{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2B - Lecture - Relevance in text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data for the next steps can be downloaded from the [Kaggle News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset).\n",
    "\n",
    "It is important to read about the dataset that you are using so that you understand what it contains and also what it doesn't contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset exploration\n",
    "\n",
    "Often we want to explore a subset of data, and we only need to analyse part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the complete dataset\n",
    "with open('data/News_Category_Dataset_v2.json', 'r') as f:\n",
    "    news_list = f.readlines()\n",
    "\n",
    "# convert each line (string) to json (dict)\n",
    "news_json = list(map(json.loads,news_list))\n",
    "\n",
    "print(\"Number of stories: \",len(news_json))\n",
    "\n",
    "# view the first 10 elements in the list\n",
    "news_json[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What categories are available in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set([story['category'] for story in news_json])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract just the science stories from the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the list for stories that are in the category SCIENCE\n",
    "science_json = [story for story in news_json if story['category']=='SCIENCE']\n",
    "\n",
    "# for each, create the 'story' by adding together the headline and the short_description\n",
    "science_stories = [story['headline']+' - '+story['short_description'] for story in science_json]\n",
    "\n",
    "print(\"Number of science stories: \",len(science_stories))\n",
    "\n",
    "# look at first 10\n",
    "science_stories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency\n",
    "\n",
    "How do we find anything meaningful in these science news stories?\n",
    "\n",
    "We could start by just extracting words and looking at the frequencies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "story1 = science_stories[0]\n",
    "\n",
    "re.split('\\W+',story1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "\n",
    "for story in science_stories:\n",
    "    words = re.split('\\W+',story.lower())\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "        \n",
    "# sort the word_counts by counts\n",
    "sorted_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "sorted_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives to finding information in text\n",
    "\n",
    "This does give us some information, but there are some problems:\n",
    "- small meaningless words are dominating the count\n",
    "- words that are most significant are spread out amongst the list\n",
    "\n",
    "The field of **Information Retrieval** has developed techniques to help with this issue. We're going to look at two...\n",
    "1. TF/IDF as a better term frequency\n",
    "2. LDA for topic modelling\n",
    "\n",
    "First we need some additional packages not installed in our Jupyter environment...\n",
    "- [gensim](https://radimrehurek.com/gensim/) for topic modelling\n",
    "- [pyLDAvis](https://github.com/bmabey/pyLDAvis) for interactive visualisation of topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install gensim\n",
    "!{sys.executable} -m pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora.textcorpus import remove_stopwords\n",
    "from gensim.summarization import keywords\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing with gensim\n",
    "\n",
    "Let's bring our stories into a dataframe and use some of the gensim tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_df = pd.DataFrame(science_stories,columns=['story'])\n",
    "stories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of tokens for first story\n",
    "tokens = list(tokenize(stories_df['story'][0],lowercase=True))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of tokens for first story using simple_preprocess\n",
    "tokens = list(simple_preprocess(stories_df['story'][0],min_len=3))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 'stopwords' from first story\n",
    "remove_stopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this for whole dataframe\n",
    "stories_df['terms'] = [remove_stopwords(simple_preprocess(story,min_len=3)) for story in stories_df['story']]\n",
    "stories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = Dictionary(stories_df['terms'])\n",
    "print(vocab.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency, Inverse Document Frequency (TF/IDF)\n",
    "\n",
    "For TF/IDF we use Bag of Words (BoW). For more information on these terms, see:\n",
    "- [A gentle introduction to the Bag-of-words model](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)\n",
    "- [tf-idf Wikipedia](https://en.wikipedia.org/wiki/Tfâ€“idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert corpus to BoW format\n",
    "corpus = [vocab.doc2bow(terms) for terms in stories_df['terms']]  \n",
    "\n",
    "# fit a tf-idf model to the corpus\n",
    "model = TfidfModel(corpus)\n",
    "\n",
    "# apply model to the first corpus document\n",
    "tfidf_doc = model[corpus[0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(vocab[w[0]],w[1]) for w in tfidf_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(vocab[w[0]],w[1]) for w in tfidf_doc if w[1]>0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_df['terms'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the second story\n",
    "terms = stories_df['terms'][1]\n",
    "print(\"terms: \",terms)\n",
    "tfidf_doc2 = model[corpus[1]]\n",
    "tfidf2 = [(vocab[w[0]],w[1]) for w in tfidf_doc2 if w[1]>0.1]\n",
    "print(\"tf/idf: \",tfidf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most relevant terms\n",
    "\n",
    "What is probably more interesting is the top n terms, which are expected to be the most relevant.\n",
    "\n",
    "Let's create a function to take the top 5 terms based on tf/idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf(idx):\n",
    "    term_values = [(vocab[el[0]],el[1]) for el in model[corpus[idx]] if el[1]>0]\n",
    "    srt =  sorted(term_values, key=lambda x: x[1],reverse=True)\n",
    "    return list(map(lambda x: x[0],srt[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tfidf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tfidf(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply this function to the whole dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_df['tfidf'] = stories_df.index.map(get_tfidf)\n",
    "stories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_df.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although TF/IDF does a good job at distinguishing between documents - identifying what is unique about a document - it doesn't use human meaning-making.\n",
    "\n",
    "Algorithmic 'semantics' is not the same as human semantics.\n",
    "\n",
    "It is worth considering how this might be a problem in a world that increasingly uses computation to process language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "However, there are approaches that are closer to human meaning-making than TF/IDF. LDA is one. For more detail on LDA, see the [LDA Wikipedia page](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an lda model from our corpus and vocab - we need to specify the number of topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=vocab, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view the topics in the model\n",
    "for topic in lda_model.show_topics(num_topics=20,num_words=15):\n",
    "    print(\"Topic \"+str(topic[0])+\"\\n\"+topic[1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document, we can get the probability that the document belongs to a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = stories_df['story'][1]\n",
    "print(\"doc:\\n\",doc)\n",
    "doc_topics = lda_model.get_document_topics(corpus[1],minimum_probability=0.3)\n",
    "print(\"doc_topics:\\n\",doc_topics)\n",
    "for topic in doc_topics:\n",
    "    terms = [term for term, prob in lda_model.show_topic(topic[0])]\n",
    "    print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function to get the top terms for the top topic for each document. This will enable us to assign the top topic words to the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_terms(idx):\n",
    "    doc_topics = lda_model.get_document_topics(corpus[idx])\n",
    "    top_topic = doc_topics[0]\n",
    "    return [term for term, prob in lda_model.show_topic(top_topic[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out the function\n",
    "get_topic_terms(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add to our original dataframe\n",
    "stories_df['lda'] = stories_df.index.map(get_topic_terms)\n",
    "stories_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us explore the model, we can visualise the topics using pyLDAvis. **NOTE:** This visualisation can take a while to produce (up to 5 minutes) so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, vocab)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
