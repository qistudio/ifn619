{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For instance, if we want to describe an apple, features such as  color or shape would be considered independent from the fruit and with different probability distributions. In this workshop, we will explore two major algorithms for training a Naive Bayes classifier: the Gaussian Naive Bayes and the MultinomialNaive Baayes (there are others, of course).\n",
    "\n",
    "Abstractly, naive Bayes is a conditional probability model: given a problem instance to be classified, represented by a vector $x =(x_{1},\\dots ,x_{n})$, representing some *N* features or pieces of evidence (independent variables), it assigns to this instance probabilities\n",
    "\n",
    "$Pr( C_k | x_1, x_2,..., x_N)$\n",
    "\n",
    "for each of K possible outcomes or classes, $C_K$.\n",
    "\n",
    "The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. We therefore reformulate the model to make it more tractable. Using Bayes' theorem, the conditional probability can be decomposed as\n",
    "\n",
    "$ Pr( C | x_1, x_2, ..., x_N ) = \\frac{Pr(C) Pr(x_1, x_2, ..., x_N | C)}{Pr( x_1, x_2, ..., x_N)} $\n",
    "\n",
    "In plain English, using Bayesian probability terminology, the above equation can be written as\n",
    "\n",
    "$ posterior = \\frac{ prior x likelihood }{evidence} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A General Machine Learning Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general supervised machine learning architecture consists in 2 major steps:\n",
    "- **A Training Phase**\n",
    "- **A Test Phase**\n",
    "\n",
    "The **training phase** consists in getting a dataset with a set of features and pass it to a machine learning system (in this week, the machine learning system that you will learn is the Naive Bayes classifier). This machine learning system will output a mathematical function, which approximates the patterns and trends of the input training data. In machine learning, this function is usually referred to as a model. This model is computed through optimization problems that try to minimize the error between each datapoint during the training phase and its correct prediction. That is whay it is called *supervised* learning: one always needs to provide information about the true predictions of the data.\n",
    "\n",
    "The **test phase** consists in the application of a set of data points that *were not used during the training phase* to the trained model, and evaluate how good that model is able to make a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ML.png\" width=\"700px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A: A Breast Cancer Classifier Using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will be applying Naive Bayes to try to predict if some tumor is malignant (cancer) or benign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Data Manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "\n",
    "# Figure Plotting libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Naive Bayes libraries\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import BernoulliNB      # Naive Bayes Classifier based on a Bernoulli Distribution\n",
    "from sklearn.naive_bayes import GaussianNB       # Naive Bayes Classifier based on a Gaussian Distribution\n",
    "from sklearn.naive_bayes import MultinomialNB    # Naive Bayes Classifier based on a Multinomial Distribution\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Text Analysis libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot gaussian distribution in data\n",
    "# you do not need to understand this in detail. Use it as a function that receives some data\n",
    "# and plots a gaussian distribution over it\n",
    "def func_plot_gaussian( data, ylim = (5, 42), xlim = (5, 30) ):\n",
    "\n",
    "    # separate the benign tumours (diagnosis = 0) from the malignant ones (diagnosis = 1)\n",
    "    mal = data[ data[ 'diagnosis'] == 1]\n",
    "    ben = data[ data[ 'diagnosis'] == 0]\n",
    "\n",
    "    # need to convert dataframe into a matrix in order to make the plot work\n",
    "    X = data[ ['radius_mean', 'texture_mean'] ]\n",
    "    x = X.to_numpy()\n",
    "\n",
    "    # plot figure\n",
    "    fig=plt.figure(dpi=150)\n",
    "    ax= fig.add_subplot(111)\n",
    "    \n",
    "    # plot the datapoints of our data and color encodede them according to their diagnosis (malignant / benign)\n",
    "    plt.scatter(mal['radius_mean'], mal['texture_mean'], c='r', marker='s', s=3, label='malignant')\n",
    "    plt.scatter(ben['radius_mean'], ben['texture_mean'], c='b', marker='o', s=3, label='benign')\n",
    "    plt.ylabel('texture_mean', fontsize=12)\n",
    "    plt.xlabel('radius_mean', fontsize=12)\n",
    "    plt.title('Breast Tumors', fontsize=14)\n",
    "    plt.legend()\n",
    "\n",
    "    # plot the gaussian curves over the data\n",
    "    # a gaussian distribution can be computed by having the mean of the data, variable mu\n",
    "    # and the standard deviation of the data, vatiable std\n",
    "    xg = np.linspace(xlim[0], xlim[1], 60)\n",
    "    yg = np.linspace(ylim[0], ylim[1], 40)\n",
    "    xx, yy = np.meshgrid(xg, yg)\n",
    "    Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "    for label, color in enumerate(['blue', 'red']):\n",
    "        mask = (y == label)\n",
    "        mu, std = x[mask].mean(0), x[mask].std(0)\n",
    "        P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1) # Gaussian distr. mathematical formula\n",
    "        Pm = np.ma.masked_array(P, P < 0.05)\n",
    "        ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5, cmap=color.title() + 's')\n",
    "        ax.contour(xx, yy, P.reshape(xx.shape), levels=[0.01, 0.1, 0.5, 0.9], colors=color, alpha=0.2) \n",
    "    \n",
    "    ax.set(xlim=xlim, ylim=ylim)\n",
    "    fig.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "# Data describes if a tumour is MALIGNANT (value 1) or BENIGN (value 0) accordong to:\n",
    "# - mean radius of the tumour\n",
    "# - mean texture of the tumour\n",
    "file_path = 'data/breast_data_simple.csv'\n",
    "data = pd.read_csv( file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st step towards a machine learning apporach: separate your dataset!\n",
    "# put the variable that you wish to classify (or predict) in one variable\n",
    "# put your sources of evidence (or your features) in another variable\n",
    "y = data['diagnosis']                        # variable to classify and preduict\n",
    "X = data[['radius_mean', 'texture_mean']]    # variable containing your features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at out variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed with any data analysis, we need to try to understand what kind of data are we dealing with. \n",
    "Naive Bayes model (like most machine learning models) are based on statistical learning. This means that the distribution of your data plays an important role in how successful the machine learning algorithm is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting purposes:\n",
    "# separate the benign tumors (diagnosis = 0) from the malignant ones (diagnosis = 1)\n",
    "malignant = data[ data[ 'diagnosis'] == 1]\n",
    "benign = data[ data[ 'diagnosis'] == 0]\n",
    "\n",
    "# need to convert dataframe into a matrix in order to make the plot work\n",
    "x = X.to_numpy()\n",
    "\n",
    "# plot figure\n",
    "fig=plt.figure(dpi=150)\n",
    "\n",
    "plt.scatter(malignant['radius_mean'], malignant['texture_mean'], c='r', marker='x', s=10, label='malignant', cmap='RdBu')\n",
    "plt.scatter(benign['radius_mean'], benign['texture_mean'], c='b', marker='o', s=10, label='benign', cmap='RdBu')\n",
    "plt.ylabel('texture_mean', fontsize=12)\n",
    "plt.xlabel('radius_mean', fontsize=12)\n",
    "plt.title('Breast Tumors', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is not sparse, which is good (it is hard to model sparse data). The data sems to be concentric and distributed around a mean value. In statistics and machine learning, we usually represent data with a Gaussian Distribution (which is nothing more than a bell shaped curve). Let's see this in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function defined in the begining of the notebook\n",
    "func_plot_gaussian( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Naive Bayes Classifier with a Gaussian Kernel\n",
    "\n",
    "Now that we took a look at our data and that we separated the data into a variable with the prediction, y, and another variable with the features, X, we need to split our data into two sets: a training set (used to estimate our model), and a test set (used to evaluate how good our model is).\n",
    "\n",
    "**Remember!** Never use the same data on your training set as your test set! Why? If you ebaluate your model using that that was used to build that model, the the algorithm will always \"know\" what is the correct prediction of that data. That is why we always test a machine learning model with a set of data points that have never been seen by the model during the training phase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Importance of Defining Test and Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set and the test set\n",
    "# good machine learning practices say that usually you should provide 70% of your data for training \n",
    "# and 30% of the data for testing\n",
    "# test_size specifies how much data do you want to reserve for the test set\n",
    "# the argument, random_state, is simply to ensure that we will have the same results\n",
    "# when we run this cell many times. Since the split between the train set and the test set is random,\n",
    "# by setting the random_state, we are ensuring reproducibility of the results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 515)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the size of our training sets and test sets\n",
    "print( \"Training set contains %d instances and the test set contains %d instances\" %(X_train.shape[0], X_test.shape[0]))\n",
    "print(\"Size of training set: %.2f\" %((X_train.shape[0]/X.shape[0])*100))\n",
    "print(\"Size of test set: %.2f\" %((X_test.shape[0]/X.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of the Type of Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the model\n",
    "\n",
    "# STEP 2: specify the learning algorithm\n",
    "# In this lecture, we will use a simple Gaussian Naive Bayes Model\n",
    "model_base = GaussianNB()\n",
    "\n",
    "# STEP 3: fit the training data to model\n",
    "model_base.fit( X_train, y_train )\n",
    "\n",
    "# STEP 4: make predictions on test set\n",
    "# given a set of features that the system did not see before\n",
    "# tries to predict the correct label to the data (label = malignant or benign tumor)\n",
    "y_prediction = model_base.predict( X_test )\n",
    "\n",
    "# # STEP 5: Measure the accuracy of the model\n",
    "# compare the predicted results with the ones associated to X_test data\n",
    "print( 'The overall accuracy of the model is %.2f%%' %(accuracy_score( y_test, y_prediction )*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, we applied the 5 steps for a machine learning problem:\n",
    "1. Split data into train and test sets\n",
    "2. Specify the learning algorithm\n",
    "3. Fit the training data to algorithm\n",
    "4. Make predictions on test set\n",
    "5. Measure the performance of the learnt model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trials = 500\n",
    "\n",
    "accuracy = []\n",
    "for trial in range( 0, trials ):\n",
    "    \n",
    "    # randomly select a test set and a training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    y_expected = y_test\n",
    "    \n",
    "    GaussNB = GaussianNB()         # create the Gaussian Naive Bayes Classifer\n",
    "    GaussNB.fit(X_train, y_train)  # fit the model to the training data\n",
    "\n",
    "    y_predicted = GaussNB.predict(X_test)                       # get predictions of model on the test set\n",
    "    accuracy.append(accuracy_score( y_expected, y_predicted ))  # save accuracy obtained in each trial\n",
    "    print(\"Applying Naive Bayes............................ Trial #\" + str(trial + 1) + \" ....... acc = \" + str( accuracy[trial] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computute overall average accuracy over the 500 trials\n",
    "min_accuracy = np.min(accuracy)\n",
    "max_accuracy = np.max(accuracy)\n",
    "avg_accuracy = np.mean( accuracy )\n",
    "\n",
    "print(\"Results range from [%.2f, %.2f]\" %(min_accuracy, max_accuracy))\n",
    "print( \"Average model accuracy is %.2f\" %avg_accuracy  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure()\n",
    "plt.scatter( range( 0, trials ), accuracy, s = 2 )\n",
    "lst = np.ones(trials, float)\n",
    "plt.plot( range( 0, trials ), avg_accuracy*lst, c='r' )\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Number of Trials', fontsize=12)\n",
    "plt.title('Average Accuracy of a Naive Bayes Classifier using a Gaussian Kernel', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print( \"Average model accuracy is %.2f\" %avg_accuracy  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we use another distribution? How about a Bernoulli distribution?\n",
    "\n",
    "A Bernoulli distribution is the kind of distribution that you get when you flip a coin many times: you get a probability *p* of a coin landing heads, and you get a probability *(1-p)* of the coin landing tails. More formally, a Bernoulli distribution is the discrete probability distribution of a random variable which takes the value 1 with probability and the value 0 with probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 500\n",
    "\n",
    "accuracy = []\n",
    "for trial in range( 0, trials ):\n",
    "    \n",
    "    # randomly select a test set and a training set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    y_expected = y_test\n",
    "    \n",
    "    # Specify the learning algorithm\n",
    "    model = BernoulliNB()   \n",
    "    \n",
    "    # Fit the training data to algorithm\n",
    "    model.fit(X_train, y_train)  # fit the model to the training data\n",
    "    \n",
    "    # Make predictions on test set using the learnt model\n",
    "    y_predicted = model.predict(X_test)\n",
    "    \n",
    "    # Measure the performance of the learnt model\n",
    "    accuracy.append(accuracy_score( y_expected, y_predicted ))  # save accuracy obtained in each trial\n",
    "    \n",
    "print( 'The average overall accuracy of the model is %.2f' %(accuracy_score( y_test, y_prediction )*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen to the performance of our classifier? How can you justify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Yourself! Breast Cancer classification with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "file_path = 'data/breast_data_full.csv'\n",
    "data_full = pd.read_csv( file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the features in this dataset? How many are there?\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "features = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate your dataset: \n",
    "# put the variable that you wish to classify (or predict) in one variable\n",
    "# put your sources of evidence (or your features) in another variable\n",
    "\n",
    "# note that your dataset contains a column id, which is not necessary. \n",
    "\n",
    "# YOUR CODE HERE:\n",
    "y = \n",
    "X = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the dataset into test set and train set\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NaiveBayes Gaussian kernel\n",
    "# YOUR CODE HERE:\n",
    "model = \n",
    "\n",
    "# Fit a model to the data -> learning the model\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "\n",
    "# Use the learned model to try to predict the tumors on the testset\n",
    "# YOUR CODE HERE:\n",
    "y_predicted =\n",
    "\n",
    "# Measure the overall accuracy of the model\n",
    "# | y_predicted - y_test | -> 0\n",
    "# YOUR CODE HERE\n",
    "accuracy = \n",
    "\n",
    "print( accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "What were your findings? Did the incorporation of more features have any impact on the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Applications: Using Naive Bayes in Text Classification\n",
    "\n",
    "We will now provide an example of how to apply a Naive Bayes classifier to news. This is a dataset of textual data where the goal is to determine the topics of each news item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity here, we will select just a few of these categories, and download the training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian', 'sci.space', 'comp.graphics']\n",
    "#categories = data.target_names\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With any of the preceding examples, it can quickly become tedious to do the transformations by hand, especially if you wish to string together multiple steps. For example, we might want a processing pipeline that looks something like this:\n",
    "\n",
    "Impute missing values using the mean\n",
    "Transform features to quadratic\n",
    "Fit a linear regression\n",
    "To streamline this type of processing pipeline, Scikit-Learn provides a Pipeline object, which can be used as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test and training sets\n",
    "X_train_raw = train.data\n",
    "y_train = train.target\n",
    "\n",
    "X_test_raw = test.data\n",
    "y_test = test.target\n",
    "y_expected = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of training instance\n",
    "print(X_train_raw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract textual features from text, such as TF.IDF (more info: https://monkeylearn.com/blog/what-is-tf-idf/)\n",
    "vec = TfidfVectorizer()\n",
    "X_train = vec.fit_transform( X_train_raw )\n",
    "X_test = vec.fit_transform( X_test_raw )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# MultinomialNB != GaussianNB\n",
    "\n",
    "\n",
    "model.fit(train.data, train.target)\n",
    "y_predicted  = model.predict(test.data)\n",
    "\n",
    "print( 'The overall accuracy of the model is %.2f%%' %(accuracy_score( y_expected, y_predicted )*100))\n",
    "\n",
    "colormap = \"YlOrBr\" # more colors can be found here: https://matplotlib.org/tutorials/colors/colormaps.html\n",
    "mat = confusion_matrix(test.target, y_predicted)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True, cmap = colormap,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing different sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Teaching data analytics with really nice graphics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Flat earth people say Australia does not exist and we are all being paid by Nasa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('Chef Kiko is opening a restaurant in Mars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category('discussing islam vs atheism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
